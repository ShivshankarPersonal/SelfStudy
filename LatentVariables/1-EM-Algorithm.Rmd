---
title: "<strong>mixture models</strong>"
author: "andrés castro araújo"
date: "`r Sys.Date()`"
output: 
  html_document: 
    code_folding: show
    theme: lumen
    toc: yes
    toc_float:
      collapsed: yes
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", fig.align = "center",
                      fig.width = 5, fig.height = 3)

library(tidyverse)
theme_set(theme_minimal(base_family = "Avenir Next Condensed", base_line_size = 0))
```


```{css, include=FALSE}
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    border-left: 5px solid #eee;
}
```

****

Everything here comes from _Modern Statistics for Modern Biology_ [-@holmes2018modern, chap. 4] by Susan Holmes and Wolgang Huber, unless stated otherwise.

****

## introduction

Simple probability distributions can be used as the building blocks for more complex and realistic statistical models.

The following graph shows a mixture of two normal distributions. We call these __finite mixtures__. Other mixtures can involve almost as many distributions as we have observations. These are called __infinite mixtures__. 

```{r}
draw_samples <- function(N = 1e3, means = c(-1, 1), sds = c(0.5, 0.5)) {
  mix <- sample(c(TRUE, FALSE), N, replace = TRUE, prob = c(0.5, 0.5))
  output <- rnorm(n = length(mix), 
        mean = ifelse(mix, means[[1]], means[[2]]),
        sd = ifelse(mix, sds[[1]], sds[[2]])
        ) 
  
  data.frame(x = output)
}

draw_samples() %>% 
  ggplot(aes(x)) + 
  geom_histogram(binwidth = 0.1, color = "steelblue", 
                 fill = "steelblue", alpha = 0.5)
```

Note that as we increase the number of observations and bins, the histogram gets nearer to a smooth curve. This smooth limiting curve is called the density function of the random variable. 

```{r}
draw_samples(N = 1e6) %>% 
  ggplot(aes(x)) + 
  geom_histogram(bins = 500, color = "steelblue", 
                 fill = "steelblue", alpha = 0.5)
```

In this case, each of the two normals can be written explicitly as

$$
\phi(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \Bigg( - \frac{1}{2} \bigg( 
\frac{x - \mu}{\sigma} \bigg)^2 \Bigg)
$$

And the mixture density in our previous example can simply be written as:

$$
\begin{align}
f(x) = \lambda_1 \ \phi_1(x) +  \lambda_2 \ \phi_2(x), && \underbrace{\lambda_1 = \lambda_2 = \frac{1}{2}}_\text{mixing proportions}
\end{align}
$$


```{r}
normal_mixture <- function(x, f1 = c(mu = -1, sd = 0.5), f2 = c(mu = 1, sd = 0.5), probs = c(0.5, 0.5)) {
    probs[[1]] * dnorm(x, f1[[1]], f1[[2]]) + 
    probs[[2]] * dnorm(x, f2[[1]], f2[[2]])
}

ggplot() +
  geom_area(aes(x = -3:3), stat = "function", fun = normal_mixture, 
    fill = "steelblue") +
  labs(y = "mixture density", x = "x")
```

****

Suppose that a dataset was generated from a mixture of two normals with the following parameters:

$$
\begin{align}
&\mu_1 = -0.5 && \mu_2 = 1.5 \\
&\sigma_1 = 0.75 && \sigma_2 = 0.75 \\
\end{align}
$$

Here is an example of a dataset generated out of such model.

```{r}
set.seed(123)
N   <- 500
mus <- c(-0.5, 1.5)
u   <- sample(1:2, N, replace = TRUE)  ## label
x   <- rnorm(N, mean = mus[u], sd = 0.75)
(df  <- tibble(u, x))
```

Number of observations in each group:

```{r}
table(df$u)
```

And because we know the labels $u$, we can estimate both means using separate maximum likelihood estimates for each group. The overall MLE is obtained by maximizing the following equation (or it's logarithm):

$$
f(x, u \mid \boldsymbol \theta) = \prod_{\{i: u_i = 1\}} \phi_1(x_i) \prod_{\{i: u_i = 2\}} \phi_2(x_i)
$$

Note that the maximization can be split into two independent pieces and solved as if we had two different MLEs to find. The MLE for the mean and variance of normal distributions are simply:

$$
\hat \mu = \frac{1}{n} \sum_{i = 1}^n x_i \hspace{1cm} \text{and} \hspace{1cm}
\hat \sigma^2 = \frac{1}{n} \sum_{i = 1}^n (x_i - \hat \mu)^2
$$

```{r}
df %>% 
  group_by(u) %>% 
  summarize(mu_hat = mean(x), sd_hat = sd(x))
```

The problem, however, is that _we won't know $u_i$ or the mixture proportions_ ( $\lambda_1, \lambda_2$). This is the problem that the __EM algorithm__ is supposed to solve. 

The next section goes more in depth into what this really means. For the time being, we can use the [__`mixtools`__](https://cran.r-project.org/web/packages/mixtools/index.html) package to provide a fast implementation of the algorithm and compare the results.

```{r}
output <- mixtools::normalmixEM(df$x)

output[c("lambda", "mu", "sigma")]
```

## the EM algorithm

The [_expectation-maximization algorithm_](https://en.wikipedia.org/wiki/Expectation–maximization_algorithm) is used to make inferences about hidden groupings (or latent variables) in data. These can be any number $K$ of groupings.

It's a popular procedure that alternates between two steps:

- Pretending we know the probability with which each observation belongs to a component (or cluster) and __estimating the distribution parameters__ of the components. 

- Pretending we know the parameters of the component (or cluster) distributions and __estimating the probability with which each observation belongs to them__. 

    We refer to these probabilities as the _weights_ of each individual data point ($w_{i,k}$).

In other words, we solve a difficult optimization problem by iteratively pretending we know one part of the solution to compute the other part.

For example, suppose we measure a variable $X$ on a series of objects. We also think that these measurements come from $K$ different groups (in this example we assume $K = 2$). We then start by _augmenting_ the data with the unobserved (or missing or latent) group label, which we call $U$. We are now interested in discovering the values of $U$, and also the unknown parameters that describe the underlying densities (e.g. $\mu_1$, $\sigma_1$, $\mu_2$, $\sigma_2$). 

After starting with initial guesses about the cluster parameters and mixing proportions (i.e. $\lambda$), we then proceed to 

1. Use the current parameter guesses to calculate the weights $w_{i,k}$ (__E-step__).

2. Use the current weights to maximize the weighted likelihood and getting new parameter estimates (__M-step__).

>These two iterations (E and M) are repeated until the improvements are small; this is a numerical indication that we are close to a flattening of the likelihood and so we have reached a local maximum. It’s good practice to repeat such a procedure several times from different starting points and check that we always get the same answer.

### implementation

```{r}
## We start with data, in this case: df$x
## And the number of clusters too look for

x <- df$x
k <- 2

# Step 1: Initial Guesses -------------------------------------------------
# Usually the mu's come from a K-Means Clustering for speed
sigma <- rep(1, k)
mu <- sample(x, k) ## two random data points serve as initial means
lambda <- rep(1 / k, k)

# E Step ------------------------------------------------------------------
# Calculate weights, assuming we know lambda, mu, and sigma

w <- map(1:k, function(k) lambda[[k]] * dnorm(x, mu[[k]], sigma[[k]]))
denominator <- purrr::reduce(w, `+`)
weights <- do.call(cbind, w) / denominator  ## see weights equation

# M Step ------------------------------------------------------------------
# Calculate lambda, mu, sigma, assuming we know weights

NK <- colSums(weights)
lambda <- NK / nrow(weights)
mu <- colSums(weights * x) / NK 
sigma <- purrr::map_dbl(1:k, function(k) sqrt(sum(weights[, k] * (x - mu[[k]])^2) / NK[[k]]))

# Likelihood --------------------------------------------------------------

ll <- sum(log(denominator))

# Convergence is generally detected by computing the value of the log-likelihood
# after each iteration and halting when it appears not to be changing in a
# significant manner from one iteration to the next

# TURN THIS INTO A WHILE LOOP

iter <- 1

cat("Iteration ", iter,
    "\nLogLikelihood:", ll,
    "\nMu:", mu,
    "\nSigma:", sigma, 
    "\nLambda: ", lambda, "\n\n")
```

Here is how Holmes and Huber [-@holmes2018modern] describe the usefulness of the EM algorithm:

>1. It shows us how we can tackle a difficult problem with too many unknowns by alternating between solving simpler problems. In this way, we eventually find estimates of hidden variables.
><br><br>
>2. It provides a first example of _soft_ averaging i.e., where we don’t decide whether an observation belongs to one group or another, but allow it to participate in several groups by using probabilities of membership as weights, and thus obtain more nuanced estimates.
><br><br>
>3. The method employed here can be extended to the more general case of __model-averaging__ (Hoeting et al. 1999). It can be sometimes beneficial to consider several models simultaneously if we are unsure which one is relevant for our data. We can combine them together into a weighted model. The weights are provided by the likelihoods of the models.

### identifiability

We should make sure that our probability distributions are [_identifiable_](https://en.wikipedia.org/wiki/Identifiability), "that if we have distinct representations of the model, they make distinct observational claims" [@shalizi2013advanced, chap. 19]. Mixture models can exhibit many issues with identifiability. For example __label degeneracy__, which means that "we can always swap the labels of any two clusters with no effect on anything observable at all --- if we decide that cluster A is now cluster number B and vice versa, that doesn’t change the distribution of $X$ at all" [@shalizi2013advanced].

__Another coin flip example:__

Suppose we have two unfair coins whose probabilities of heads are $p_1 = 0.125$ and $p_2 = 0.25$. We pick coin 1 with probability $\lambda$ (and coin 2 with probability $1-\lambda$). After picking a coin, we toss it twice and record the number of heads $K$.

```{r}
set.seed(911)
coin_simulation <- function(N, p1, p2, lambda) {
  coin <- sample(c(1, 2), size = N, replace = TRUE, prob = c(lambda, 1 - lambda))
  K <- rbinom(n = length(coin), size = 2, prob = ifelse(coin == 1, p1, p2))
  return(K)
}

table(coin_simulation(N = 100, p1 = 1/8, p2 = 1/4, lambda = 1/8))
table(coin_simulation(N = 100, p1 = 1/8, p2 = 1/4, lambda = 1/4))
```

After seeing both contingency tables, can we uniquely estimate the values of $p_1$, $p_2$, and $\lambda$? This seems _very unlikely_.

More generally, the problem of identifiability arises when there are too many degrees of freedom in the parameters.

### zero inflated data

Mixture models are useful whenever we have observations that can be related to different causes. Note that these models can incorporate different probability distributions to model the same outcome $y$. 

>Count variables are especially prone to needing a mixture treatment. The reason is that a count of zero can often arise more than one way. A "zero" means that nothing happened, and nothing can happen either because the rate of events is low or rather because the process that generates events failed to get started [@mcelreath2015statistical].

The zero inflated model will usually look something like this:

$$
f_{ZI}(y) = \lambda \ \delta(y) + (1 - \lambda) \ f_{\text{count}}(y)
$$
Here, the $\delta$ is Dirac's delta function, which represents a probability distribution that has all its mass at 0. 

## bootstrap


The sampling distribution is the set of possible data sets that could have been observed, if the data collection process had been repeated many many times. The true sampling distribution of a statistic (e.g. mean, variance, etc) is often hard to know as it requires many different data samples. 


We use the standard error –which is nothing more than the estimated standard deviation of the sampling distribution– to characterize the variability of an estimator.

```{r}
sim <- tibble(dist = replicate(1e5, mean(rnorm(n = 100, mean = 1, sd = 2))))

one_draw <- rnorm(n = 100, mean = 1, sd = 2)
ggplot(sim, aes(x = dist)) + 
  geom_density(color = "steelblue", fill = "steelblue", alpha = 0.5) +
  geom_vline(xintercept = mean(one_draw), color = "tomato", linetype = "dashed")

## bootstrap
B <- 5000
bmeans <- replicate(B, {
  i = sample(100, 100, replace = TRUE)
  sd(one_draw[i])
})

ggplot(NULL, aes(x = bmeans)) + 
  geom_density(color = "steelblue", fill = "steelblue", alpha = 0.5) +
  geom_vline(xintercept = median(one_draw), color = "tomato", linetype = "dashed")

bmeans %>% mean
sd(one_draw) / sqrt(length(one_draw))
```

```{r}
HistData::ZeaMays
```


```{r}
B <- 5000
meds <- replicate(B, {
  i = sample(15, 15, replace = TRUE)
  median(HistData::ZeaMays$diff[i])
})

ggplot(tibble(medians = meds), aes(x = medians)) +
  geom_histogram(bins = 30, fill = "purple") 
  geom_vline(xintercept = HistData::ZeaMays$diff %>% mean)


```




## infinite mixtures

## References