---
title: "The EM Algorithm"
author: "Andrés Castro Araújo"
date: "`r Sys.Date()`"
output: 
  html_document: 
    code_folding: hide
    theme: lumen
    toc: yes
    toc_float: 
      collapsed: yes
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", fig.align = "center",
                      fig.width = 4, fig.height = 3)

library(tidyverse)
theme_set(theme_minimal(base_family = "Avenir", base_line_size = 0))
```

****

Everything here comes from _Modern Statistics for Modern Biology_ [-@holmes2018modern, chap. 4] by Susan Holmes and Wolgang Huber, unless stated otherwise.

****

## Mixture Models

Simple probability distributions can be used as the building blocks for more complex and realistic statistical models.

The following graph shows a mixture of two normal distributions. We call these _finite mixtures_. Other mixtures can involve almost as many distributions as we have observations. These we call _infinite mixtures_. 

```{r}
N <- 1e3
mix <- sample(c(TRUE, FALSE), N, replace = TRUE, prob = c(0.5, 0.5))
draw_samples <- function(x, means = c(-1, 1), sds = c(0.5, 0.5)) {
  rnorm(n = length(x), 
        mean = ifelse(x, means[[1]], means[[2]]),
        sd = ifelse(x, sds[[1]], sds[[2]])
        )
}

ggplot(tibble(value = draw_samples(mix)), aes(x = value)) + 
  geom_histogram(binwidth = 0.1, color = "steelblue1", fill = "steelblue1", alpha = 0.5)
```

Note that as we increase the number of observations and bins, the histogram gets nearer to a smooth curve. This smooth limiting curve is called the density function of the random variable. 

```{r}
N <- 1e6
mix <- sample(c(TRUE, FALSE), N, replace = TRUE, prob = c(0.5, 0.5))
ggplot(tibble(value = draw_samples(mix)), aes(x = value)) + 
  geom_histogram(bins = 500, color = "steelblue1", fill = "steelblue1", alpha = 0.5)
```

In this case, each of the two normals that can be written explicitly as

$$
\phi(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \Bigg( - \frac{1}{2} \bigg( 
\frac{x - \mu}{\sigma} \bigg)^2 \Bigg)
$$

And the mixture density in our previous example can simply be written as:

$$
f(x) = \frac{1}{2} \phi_1(x) + \frac{1}{2} \phi_2(x)
$$

```{r}
f_mixture <- function(x, f1 = c(mu = -1, sd = 0.5), f2 = c(mu = 1, sd = 0.5), probs = c(0.5, 0.5)) {
    probs[[1]] * dnorm(x, f1[[1]], f1[[2]]) + 
    probs[[2]] * dnorm(x, f2[[1]], f2[[2]])
}

ggplot(tibble(x = c(-3, 3)), aes(x)) +
  geom_area(stat = "function", fun = f_mixture, fill = "steel blue1") +
  labs(y = "mixture density")
```

## The EM Algorithm

We can use the [_expectation-maximization algorithm_](https://en.wikipedia.org/wiki/Expectation–maximization_algorithm) to make inferences about hidden groupings (or latent variables) in our data. 


The EM algorithm is a popular procedure that alternates between two steps [@holmes2018modern]:

1. pretending we know the probability with which each observation belongs to a component (or cluster) and estimating the distribution parameters of the components, and

2. pretending we know the parameters of the component (or cluster) distributions and estimating the probability with which each observation belongs to them.

The name "EM" comes from those two steps. 

After starting with initial guesses about the cluster parameters and mixing proportions, we (1) calculate the weights using those guesses

We first find the "expectation" of the 


REWRITE, LOOK AT EVERYTHING


GELMAN:
The name ‘EM’ comes from the two alternating steps: finding the expectation of the
needed functions (the sufficient statistics) of the missing values, and maximizing the resulting posterior density to estimate the parameters as if these functions of the missing data
were observed.

For example, suppose we measure a variable $Y$ on a series of objects. We think these measurements come from two different groups, but we don't know for sure. We then start by _augmenting_ the data with the unobserved (or latent) group label, which we call $U$. We are now interested in discovering the values of $U$, and also the unknown parameters that describe the underlying densities (e.g. $\mu_1$, $\sigma_1$, $\mu_2$, $\sigma_2$). 

### Identifiability

Before attempting to estimate our probability distributions, we need to make sure they are [_identifiable_](https://en.wikipedia.org/wiki/Identifiability),  "that if we have distinct representations of the model, they make distinct observational claims" [@shalizi2013advanced, chap. 19]. Mixture models can exhibit many issues with identifiability. For example __label degeneracy__, which means that "we can always swap the labels of any two clusters with no effect on anything observable at all --- if we decide that cluster A is now cluster number B and vice versa, that doesn’t
change the distribution of $X$ at all" [@shalizi2013advanced].

Another coin flip example:

We have two unfair coins, whose probabilities of heads are $p_1 = 0.125$ and $p_2 = 0.25$. We pick coin 1 with probability $\lambda$ (and coin 2 with probability $1-\lambda$). After picking a coin, we toss it twice and record the number of heads $K$.

```{r}
set.seed(911)
coin_simulation <- function(N, p1, p2, lambda) {
  coin <- sample(c(1, 2), size = N, replace = TRUE, prob = c(lambda, 1 - lambda))
  K <- rbinom(n = N, size = 2, prob = ifelse(coin == 1, p1, p2))
  return(K)
}

table(coin_simulation(N = 100, p1 = 1/8, p2 = 1/4, lambda = 1/8))
table(coin_simulation(N = 100, p1 = 1/8, p2 = 1/4, lambda = 1/4))
```

After seeing both contingency tables, do you feel that you could uniquely estimate the values of $p_1$, $p_2$, and $\lambda$? 

More generally, the problem of identifiability arises when there are too many degrees of freedom in the parameters.

### Mixture of normals

Back to our motivating example, the mixture of normals.

Suppose that a dataset was generated from a mixture of two normals with the following parameters:

$$
\begin{align}
&\mu_1 = -0.5 && \mu_2 = 1.5 \\
&\sigma_1 = 1 && \sigma_2 = 1 \\
\end{align}
$$

Here is an example of a dataset generated out of such model.

```{r}
N   <- 100
mus <- c(-0.5, 1.5)
u   <- sample(1:2, N, replace = TRUE)  ## label
y   <- rnorm(N, mean = mus[u])
df  <- tibble(u, y)

glimpse(df)
```

And because we know the labels $u$, we can estimate both means using separate maximum likelihood estimates for each group.

The overall MLE is obtained by maximizing the following equation (or it's logarithm):

$$
f(y, u \mid \boldsymbol \theta) = \prod_{\{i: u_i = 1\}} \phi_1(y_i) \prod_{\{i: u_i = 2\}} \phi_2(y_i)
$$

Note that the MLE for the mean and variance of normal distributions are simply:

$$
\hat \mu = \frac{1}{n} \sum_{i = 1}^n x_i
$$

$$
\hat \sigma^2 = \frac{1}{n} \sum_{i = 1}^n (x_i - \hat \mu)^2
$$

```{r}
df %>% 
  group_by(u) %>% 
  summarize(mu_hat = mean(y), sd_hat = sd(y))
```

The problem, however, is that we won't know $u$ or the mixture proportions (i.e. $\lambda$). This is the problem that the EM algorithm is supposed to solve.

We can use the [__`mixtools`__](https://cran.r-project.org/web/packages/mixtools/index.html) package to provide a fast implementation of this algorithm and compare the results.

```{r}
output <- mixtools::normalmixEM(df$y)
output$lambda
```

These are the new MLE estimators:

```{r}
output$mu
output$sigma
```

Summary

REWRITE

It shows us how we can tackle a difficult problem with too many unknowns by alternating between solving simpler problems. In this way, we eventually find estimates of hidden variables.

It provides a first example of soft averaging i.e., where we don’t decide whether an observation belongs to one group or another, but allow it to participate in several groups by using probabilities of membership as weights, and thus obtain more nuanced estimates.

The method employed here can be extended to the more general case of model-averaging (Hoeting et al. 1999). It can be sometimes beneficial to consider several models simultaneously if we are unsure which one is relevant for our data. We can combine them together into a weighted model. The weights are provided by the likelihoods of the models.

## Bootstrap

## Infinite Mixtures

## References